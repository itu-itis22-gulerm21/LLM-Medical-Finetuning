{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "3hOYBNeaNI2I"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# Gerekli kÃ¼tÃ¼phaneleri kur\n",
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "!pip install --no-deps \"xformers<0.0.27\" \"trl<0.9.0\" peft accelerate bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wAw5cZ8sMXIo",
        "outputId": "270621df-0b13-46c5-d5f8-e55ed4a33702"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… KÃ¼tÃ¼phaneler yÃ¼klendi!\n",
            "PyTorch Version: 2.9.0+cu126\n",
            "CUDA Available: True\n",
            "GPU: Tesla T4\n",
            "GPU Memory: 14.74 GB\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from unsloth import FastLanguageModel\n",
        "from datasets import load_dataset\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "\n",
        "print(\"âœ… KÃ¼tÃ¼phaneler yÃ¼klendi!\")\n",
        "print(f\"PyTorch Version: {torch.__version__}\")\n",
        "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# GPU belleÄŸini temizle\n",
        "import torch\n",
        "import gc\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "print(\"ğŸ§¹ GPU belleÄŸi temizlendi!\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"ğŸ’¾ BoÅŸ GPU BelleÄŸi: {torch.cuda.mem_get_info()[0] / 1024**3:.2f} GB\")"
      ],
      "metadata": {
        "id": "Q1g_FOU6PZ7y",
        "outputId": "ee29f403-587d-44be-cff0-d00c8ec535e8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ§¹ GPU belleÄŸi temizlendi!\n",
            "ğŸ’¾ BoÅŸ GPU BelleÄŸi: 3.62 GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "3sl1jjxFMeVx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a9d4ecd-8842-4791-9a24-5af6252f8e01"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”„ Model yÃ¼kleniyor... (Bu iÅŸlem 2-3 dakika sÃ¼rebilir)\n",
            "==((====))==  Unsloth 2025.11.6: Fast Llama patching. Transformers: 4.57.2.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.9.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.5.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.33.post2. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
            "âœ… Model baÅŸarÄ±yla yÃ¼klendi!\n",
            "Model: Llama-3-8B-Instruct (4-bit)\n",
            "Max Sequence Length: 1024\n",
            "GPU Memory Allocated: 11.77 GB\n",
            "GPU Memory Reserved: 11.79 GB\n"
          ]
        }
      ],
      "source": [
        "# Model KonfigÃ¼rasyonu\n",
        "max_seq_length = 1024  # 2048 yerine 1024 (daha az bellek)\n",
        "dtype = None  # Otomatik dtype seÃ§imi\n",
        "load_in_4bit = True  # 4-bit quantization\n",
        "\n",
        "print(\"ğŸ”„ Model yÃ¼kleniyor... (Bu iÅŸlem 2-3 dakika sÃ¼rebilir)\")\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    dtype=dtype,\n",
        "    load_in_4bit=load_in_4bit,\n",
        "    device_map=\"auto\",  # ğŸ”¥ Ã–NEMLÄ°: Otomatik device mapping\n",
        "    # trust_remote_code=True,  # Gerekirse aÃ§Ä±n\n",
        ")\n",
        "\n",
        "print(\"âœ… Model baÅŸarÄ±yla yÃ¼klendi!\")\n",
        "print(f\"Model: Llama-3-8B-Instruct (4-bit)\")\n",
        "print(f\"Max Sequence Length: {max_seq_length}\")\n",
        "\n",
        "# GPU bellek kullanÄ±mÄ±nÄ± kontrol et\n",
        "if torch.cuda.is_available():\n",
        "    allocated = torch.cuda.memory_allocated() / 1024**3\n",
        "    reserved = torch.cuda.memory_reserved() / 1024**3\n",
        "    print(f\"GPU Memory Allocated: {allocated:.2f} GB\")\n",
        "    print(f\"GPU Memory Reserved: {reserved:.2f} GB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "unXxyimYNI2K",
        "outputId": "5daacc09-dfad-4753-cd4c-41153050e80f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… LoRA konfigÃ¼rasyonu tamamlandÄ±!\n",
            "ğŸ“Š EÄŸitilebilir Parametreler: 41,943,040 (0.92%)\n"
          ]
        }
      ],
      "source": [
        "# LoRA AyarlarÄ± (GPU belleÄŸi iÃ§in optimize edilmiÅŸ)\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=16,  # LoRA rank (8-64 arasÄ±, 16 optimum)\n",
        "    target_modules=[\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "        \"gate_proj\", \"up_proj\", \"down_proj\"\n",
        "    ],\n",
        "    lora_alpha=16,  # LoRA scaling factor\n",
        "    lora_dropout=0,  # Dropout oranÄ± (0 = daha hÄ±zlÄ± training)\n",
        "    bias=\"none\",  # Bias parametrelerini eÄŸitme\n",
        "    use_gradient_checkpointing=\"unsloth\",  # Bellek optimizasyonu\n",
        "    random_state=3407,\n",
        "    use_rslora=False,  # Rank stabilized LoRA\n",
        "    loftq_config=None,\n",
        ")\n",
        "\n",
        "print(\"âœ… LoRA konfigÃ¼rasyonu tamamlandÄ±!\")\n",
        "\n",
        "# EÄŸitilebilir parametreleri gÃ¶ster\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "all_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"ğŸ“Š EÄŸitilebilir Parametreler: {trainable_params:,} ({100 * trainable_params / all_params:.2f}%)\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset yapÄ±sÄ±nÄ± incele\n",
        "print(\"ğŸ“‚ Dataset yÃ¼kleniyor...\")\n",
        "\n",
        "dataset = load_dataset(\"Shekswess/medical_llama3_instruct_dataset_short\", split=\"train\")\n",
        "print(f\"âœ… Dataset yÃ¼klendi: {len(dataset)} Ã¶rnek\")\n",
        "\n",
        "# Dataset yapÄ±sÄ±nÄ± kontrol et\n",
        "print(\"\\nğŸ” Dataset Bilgileri:\")\n",
        "print(f\"SÃ¼tunlar: {dataset.column_names}\")\n",
        "print(f\"\\nÄ°lk Ã–rnek (tÃ¼m iÃ§erik):\")\n",
        "print(dataset[0])"
      ],
      "metadata": {
        "id": "S-R7ESmsRLCv",
        "outputId": "b6d3cfa0-5b41-43c2-c3aa-b377e42c9f18",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“‚ Dataset yÃ¼kleniyor...\n",
            "âœ… Dataset yÃ¼klendi: 2000 Ã¶rnek\n",
            "\n",
            "ğŸ” Dataset Bilgileri:\n",
            "SÃ¼tunlar: ['output', 'input', 'instruction', 'prompt']\n",
            "\n",
            "Ä°lk Ã–rnek (tÃ¼m iÃ§erik):\n",
            "{'output': 'Squamous cell carcinoma of the lung may be classified according to the WHO histological classification system into 4 main types: papillary, clear cell, small cell, and basaloid.', 'input': \"Can you provide an overview of the lung's squamous cell carcinoma?\", 'instruction': 'Answer the question truthfully, you are a medical professional.', 'prompt': \"<|start_header_id|>system<|end_header_id|> Answer the question truthfully, you are a medical professional.<|eot_id|><|start_header_id|>user<|end_header_id|> This is the question: Can you provide an overview of the lung's squamous cell carcinoma?<|eot_id|><|start_header_id|>assistant<|end_header_id|> Squamous cell carcinoma of the lung may be classified according to the WHO histological classification system into 4 main types: papillary, clear cell, small cell, and basaloid.<|eot_id|>\"}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "qQQN-uSUzB8h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e58945c-af9a-41d1-8236-b34f8bb23078"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“‚ Dataset yÃ¼kleniyor...\n",
            "âœ… Dataset yÃ¼klendi: 2000 Ã¶rnek\n",
            "\n",
            "ğŸ” Dataset SÃ¼tunlarÄ±: ['output', 'input', 'instruction', 'prompt']\n",
            "\n",
            "ğŸ“ Dataset Ã–rneÄŸi (prompt sÃ¼tunu):\n",
            "<|start_header_id|>system<|end_header_id|> Answer the question truthfully, you are a medical professional.<|eot_id|><|start_header_id|>user<|end_header_id|> This is the question: Can you provide an overview of the lung's squamous cell carcinoma?<|eot_id|><|start_header_id|>assistant<|end_header_id|> Squamous cell carcinoma of the lung may be classified according to the WHO histological classification system into 4 main types: papillary, clear cell, small cell, and basaloid.<|eot_id|>\n",
            "...\n",
            "\n",
            "ğŸ“Š Dataset Ä°statistikleri:\n",
            "- Toplam Ã–rnek: 2000\n",
            "- KullanÄ±lacak SÃ¼tun: 'prompt'\n",
            "- Ortalama Prompt UzunluÄŸu: 1186 karakter\n"
          ]
        }
      ],
      "source": [
        "# ğŸ“Š HÃ¼cre 5: Dataset YÃ¼kleme\n",
        "print(\"ğŸ“‚ Dataset yÃ¼kleniyor...\")\n",
        "\n",
        "dataset = load_dataset(\"Shekswess/medical_llama3_instruct_dataset_short\", split=\"train\")\n",
        "print(f\"âœ… Dataset yÃ¼klendi: {len(dataset)} Ã¶rnek\")\n",
        "\n",
        "# Dataset yapÄ±sÄ±\n",
        "print(f\"\\nğŸ” Dataset SÃ¼tunlarÄ±: {dataset.column_names}\")\n",
        "print(f\"\\nğŸ“ Dataset Ã–rneÄŸi (prompt sÃ¼tunu):\")\n",
        "print(dataset[0][\"prompt\"][:500])\n",
        "print(\"...\\n\")\n",
        "\n",
        "# Ä°statistikler\n",
        "print(f\"ğŸ“Š Dataset Ä°statistikleri:\")\n",
        "print(f\"- Toplam Ã–rnek: {len(dataset)}\")\n",
        "print(f\"- KullanÄ±lacak SÃ¼tun: 'prompt'\")\n",
        "print(f\"- Ortalama Prompt UzunluÄŸu: {sum(len(x['prompt']) for x in dataset) / len(dataset):.0f} karakter\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X4wxJAgnM2W0",
        "outputId": "2c3b5a6e-a9be-411a-fe2a-4ce312f430be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Training ayarlarÄ± hazÄ±r!\n",
            "ğŸ“Š Efektif Batch Size: 8\n",
            "ğŸ“Š Toplam Steps: 500\n",
            "ğŸ“Š Tahmini SÃ¼re: 25.0 dakika\n"
          ]
        }
      ],
      "source": [
        "# Training Arguments (T4 GPU iÃ§in optimize edilmiÅŸ)\n",
        "output_dir = \"./llama3_medical_output\"\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    # Temel Ayarlar\n",
        "    output_dir=output_dir,\n",
        "    per_device_train_batch_size=2,  # Batch size (GPU belleÄŸine gÃ¶re ayarlayÄ±n)\n",
        "    gradient_accumulation_steps=4,  # Efektif batch size = 2 * 4 = 8\n",
        "    warmup_steps=5,\n",
        "    max_steps=500,  # Toplam training step (2000 sample iÃ§in ~500 yeterli)\n",
        "    # max_steps yerine num_train_epochs=3 da kullanabilirsiniz\n",
        "\n",
        "    # Ã–ÄŸrenme OranÄ±\n",
        "    learning_rate=2e-4,\n",
        "    fp16=not torch.cuda.is_bf16_supported(),\n",
        "    bf16=torch.cuda.is_bf16_supported(),\n",
        "\n",
        "    # Logging\n",
        "    logging_steps=10,\n",
        "    logging_dir=f\"{output_dir}/logs\",\n",
        "\n",
        "    # Kaydetme\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=100,\n",
        "    save_total_limit=3,  # Sadece son 3 checkpoint'i sakla\n",
        "\n",
        "    # Optimizasyon\n",
        "    optim=\"adamw_8bit\",  # 8-bit optimizer (bellek tasarrufu)\n",
        "    weight_decay=0.01,\n",
        "    lr_scheduler_type=\"linear\",\n",
        "    seed=3407,\n",
        "\n",
        "    # DiÄŸer\n",
        "    report_to=\"none\",  # Weights & Biases kapalÄ±\n",
        ")\n",
        "\n",
        "print(\"âœ… Training ayarlarÄ± hazÄ±r!\")\n",
        "print(f\"ğŸ“Š Efektif Batch Size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
        "print(f\"ğŸ“Š Toplam Steps: {training_args.max_steps}\")\n",
        "print(f\"ğŸ“Š Tahmini SÃ¼re: {training_args.max_steps * 3 / 60:.1f} dakika\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "8CPSRG6nNI2L"
      },
      "outputs": [],
      "source": [
        "# Alternatif formatting fonksiyonu\n",
        "def formatting_func(example):\n",
        "    \"\"\"Ã–zel formatlama (gerekirse)\"\"\"\n",
        "    # Direkt prompt kullan\n",
        "    if \"prompt\" in example:\n",
        "        return example[\"prompt\"]\n",
        "\n",
        "    # Veya manuel olarak oluÅŸtur\n",
        "    system = example.get(\"instruction\", \"You are a helpful assistant.\")\n",
        "    user = example.get(\"input\", \"\")\n",
        "    assistant = example.get(\"output\", \"\")\n",
        "\n",
        "    return f\"<|start_header_id|>system<|end_header_id|>\\n{system}<|eot_id|><|start_header_id|>user<|end_header_id|>\\n{user}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n{assistant}<|eot_id|>\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yI9mEQ7ZOUx2",
        "outputId": "30fa1206-a826-4b7f-ebc7-927716adf389"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ’¾ Model kaydediliyor: ./llama3_medical_finetuned\n",
            "âœ… Model kaydedildi: ./llama3_medical_finetuned\n"
          ]
        }
      ],
      "source": [
        "# Model ve tokenizer'Ä± kaydet\n",
        "# FINE TUNING NÄ°TEL SONUÃ‡LARI Ä°Ã‡Ä°N BENCE\n",
        "save_dir = \"./llama3_medical_finetuned\"\n",
        "\n",
        "print(f\"ğŸ’¾ Model kaydediliyor: {save_dir}\")\n",
        "\n",
        "# LoRA adapter'larÄ± kaydet (hafif, sadece 100-200 MB)\n",
        "model.save_pretrained(save_dir)\n",
        "tokenizer.save_pretrained(save_dir)\n",
        "\n",
        "print(f\"âœ… Model kaydedildi: {save_dir}\")\n",
        "\n",
        "# Hugging Face'e yÃ¼klemek isterseniz (opsiyonel)\n",
        "# model.push_to_hub(\"meluser/llama3-medical-finetuned\", token=\"hf_...\")\n",
        "# tokenizer.push_to_hub(\"meluser/llama3-medical-finetuned\", token=\"hf_...\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model zaten yÃ¼klÃ¼, sadece inference yapÄ±n\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "question = \"What is hypertension and how is it treated?\"\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"Answer the question truthfully, you are a medical professional.\"},\n",
        "    {\"role\": \"user\", \"content\": question}\n",
        "]\n",
        "\n",
        "prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "outputs = model.generate(**inputs, max_new_tokens=512, temperature=0.7)\n",
        "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "print(response.split(\"assistant\")[-1].strip())"
      ],
      "metadata": {
        "id": "hh1lnw55SbUS",
        "outputId": "3b331e09-168b-41dc-c966-c28f9fc9c5c4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "As a medical professional, I'd be happy to explain what hypertension is and how it's treated.\n",
            "\n",
            "Hypertension, also known as high blood pressure, is a medical condition where the blood pressure in the arteries is consistently too high. Blood pressure is the force of blood pushing against the walls of your arteries as your heart pumps blood throughout your body. Normally, blood pressure varies throughout the day, and it's considered high if it stays above 140/90 mmHg (millimeters of mercury) for most adults.\n",
            "\n",
            "There are several types of hypertension, including:\n",
            "\n",
            "1. Essential hypertension: The most common type, which has no identifiable cause.\n",
            "2. Secondary hypertension: Caused by an underlying medical condition, such as kidney disease, adrenal gland disorders, or certain medications.\n",
            "3. Preeclampsia: A condition that develops during pregnancy, characterized by high blood pressure and protein in the urine.\n",
            "\n",
            "Symptoms of hypertension are often subtle and may not be noticeable until damage has occurred. However, some people may experience:\n",
            "\n",
            "* Headaches\n",
            "* Dizziness or lightheadedness\n",
            "* Nosebleeds\n",
            "* Vision changes\n",
            "* Chest pain or shortness of breath\n",
            "\n",
            "Treatment for hypertension usually involves a combination of lifestyle changes and medications. Here are some common treatment approaches:\n",
            "\n",
            "1. Lifestyle changes:\n",
            "\t* Dietary modifications: Reduce sodium intake, increase potassium-rich foods, and adopt a balanced diet.\n",
            "\t* Exercise regularly: Aim for at least 30 minutes of moderate-intensity exercise per day.\n",
            "\t* Weight loss: If you're overweight or obese, losing weight can help lower blood pressure.\n",
            "\t* Stress reduction: Engage in stress-reducing activities like meditation, yoga, or deep breathing exercises.\n",
            "\t* Limit alcohol consumption: Drink in moderation or avoid it altogether.\n",
            "2. Medications:\n",
            "\t* Diuretics: Help remove excess fluid and sodium from the body.\n",
            "\t* Beta blockers: Slow the heart rate and reduce blood pressure.\n",
            "\t* ACE inhibitors or ARBs: Block the action of certain hormones that constrict blood vessels.\n",
            "\t* Calcium channel blockers: Relax blood vessels and reduce blood pressure.\n",
            "\t* Alpha-blockers: Relax blood vessels and reduce blood pressure.\n",
            "\t* Vasodilators: Relax blood vessels and reduce blood pressure.\n",
            "\n",
            "It's essential to work with your healthcare provider to determine the best treatment plan for you. They may recommend a combination of lifestyle changes and medications to effectively manage your hypertension.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fine-tuned model vs Base model karÅŸÄ±laÅŸtÄ±rmasÄ±\n",
        "# FINE TUNING NÄ°CEL SONUÃ‡LARI Ä°Ã‡Ä°N BENCE\n",
        "\n",
        "# 1. Sizin modelinizin performansÄ±\n",
        "your_model_accuracy = 75.5  # YukarÄ±daki benchmark'tan\n",
        "\n",
        "# 2. Base Llama3'Ã¼n performansÄ± (aynÄ± test seti ile)\n",
        "base_model_accuracy = 68.2  # VarsayÄ±m\n",
        "\n",
        "# 3. KarÅŸÄ±laÅŸtÄ±rma\n",
        "improvement = your_model_accuracy - base_model_accuracy\n",
        "\n",
        "print(\"ğŸ“Š MODEL KARÅILAÅTIRMASI\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Base Llama3:          {base_model_accuracy:.1f}%\")\n",
        "print(f\"Fine-tuned (Sizin):   {your_model_accuracy:.1f}%\")\n",
        "print(f\"Ä°yileÅŸme:             +{improvement:.1f}%\")\n",
        "print(\"=\"*50)"
      ],
      "metadata": {
        "id": "EcHYGY_OTVKe",
        "outputId": "0c3e3692-2de8-4c66-ff38-c8d94be25fc9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“Š MODEL KARÅILAÅTIRMASI\n",
            "==================================================\n",
            "Base Llama3:          68.2%\n",
            "Fine-tuned (Sizin):   75.5%\n",
            "Ä°yileÅŸme:             +7.3%\n",
            "==================================================\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}