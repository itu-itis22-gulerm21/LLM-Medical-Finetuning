{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ooiBrhAEd71X"
      },
      "outputs": [],
      "source": [
        "# Installing all the necessary packages(libraries) for the experiments (RTX30xx, RTX40xx, A100, H100, L40)\n",
        "!pip install --no-deps packaging ninja einops flash-attn xformers trl peft accelerate bitsandbytes\n",
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wAw5cZ8sMXIo"
      },
      "outputs": [],
      "source": [
        "# Installing all the necessary packages(libraries) for the experiments (V100, T4, RTX20xx)\n",
        "!pip install --no-deps xformers trl peft accelerate bitsandbytes\n",
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3sl1jjxFMeVx"
      },
      "outputs": [],
      "source": [
        "# Importing the necessary packages(libraries) for the experiments\n",
        "import json\n",
        "\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from huggingface_hub import notebook_login\n",
        "from transformers import TrainingArguments\n",
        "from trl import SFTTrainer\n",
        "from unsloth import FastLanguageModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w6BPig94d71a"
      },
      "outputs": [],
      "source": [
        "# Logging into the Hugging Face Hub(with token)\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qQQN-uSUzB8h"
      },
      "outputs": [],
      "source": [
        "# Defining the configuration for the base model, LoRA and training\n",
        "config = {\n",
        "    \"hugging_face_username\":\"Shekswess\",\n",
        "    \"model_config\": {\n",
        "        \"base_model\":\"unsloth/gemma-1.1-7b-it-bnb-4bit\", # The base model\n",
        "        \"finetuned_model\":\"meluser/gemma-1.1-7b-it-bnb-4bit-medical\", # The finetuned model\n",
        "        \"max_seq_length\": 2048, # The maximum sequence length\n",
        "        \"dtype\":torch.float16, # The data type\n",
        "        \"load_in_4bit\": True, # Load the model in 4-bit\n",
        "    },\n",
        "    \"lora_config\": {\n",
        "      \"r\": 16, # The number of LoRA layers 8, 16, 32, 64\n",
        "      \"target_modules\": [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\"], # The target modules\n",
        "      \"lora_alpha\":16, # The alpha value for LoRA\n",
        "      \"lora_dropout\":0, # The dropout value for LoRA\n",
        "      \"bias\":\"none\", # The bias for LoRA\n",
        "      \"use_gradient_checkpointing\":True, # Use gradient checkpointing\n",
        "      \"use_rslora\":False, # Use RSLora\n",
        "      \"use_dora\":False, # Use DoRa\n",
        "      \"loftq_config\":None # The LoFTQ configuration\n",
        "    },\n",
        "    \"training_dataset\":{\n",
        "        \"name\":\"Shekswess/medical_gemma_instruct_dataset_short\", # The dataset name(huggingface/datasets)\n",
        "        \"split\":\"train\", # The dataset split\n",
        "        \"input_field\":\"prompt\", # The input field\n",
        "    },\n",
        "    \"training_config\": {\n",
        "        \"report_to\": \"none\",\n",
        "        \"per_device_train_batch_size\": 2, # The batch size\n",
        "        \"gradient_accumulation_steps\": 4, # The gradient accumulation steps\n",
        "        \"warmup_steps\": 5, # The warmup steps\n",
        "        \"max_steps\":0, # The maximum steps (0 if the epochs are defined)\n",
        "        \"num_train_epochs\": 1, # The number of training epochs(0 if the maximum steps are defined)\n",
        "        \"learning_rate\": 2e-4, # The learning rate\n",
        "        \"fp16\": not torch.cuda.is_bf16_supported(), # The fp16\n",
        "        \"bf16\": torch.cuda.is_bf16_supported(), # The bf16\n",
        "        \"logging_steps\": 1, # The logging steps\n",
        "        \"optim\" :\"adamw_8bit\", # The optimizer\n",
        "        \"weight_decay\" : 0.01,  # The weight decay\n",
        "        \"lr_scheduler_type\": \"linear\", # The learning rate scheduler\n",
        "        \"seed\" : 42, # The seed\n",
        "        \"output_dir\" : \"outputs\", # The output directory\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "id": "X4wxJAgnM2W0",
        "outputId": "fda5fae5-1b95-4f0c-f6a2-f88317a0a3c0"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'FastLanguageModel' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1780042563.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Loading the model and the tokinizer for the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m model, tokenizer = FastLanguageModel.from_pretrained(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mmodel_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"model_config\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"base_model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mmax_seq_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"model_config\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"max_seq_length\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"model_config\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dtype\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'FastLanguageModel' is not defined"
          ]
        }
      ],
      "source": [
        "# Loading the model and the tokenizer for the model\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = config.get(\"model_config\").get(\"base_model\"),\n",
        "    max_seq_length = config.get(\"model_config\").get(\"max_seq_length\"),\n",
        "    dtype = config.get(\"model_config\").get(\"dtype\"),\n",
        "    load_in_4bit = config.get(\"model_config\").get(\"load_in_4bit\"),\n",
        ")\n",
        "\n",
        "# Setup for QLoRA/LoRA peft of the base model\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = config.get(\"lora_config\").get(\"r\"),\n",
        "    target_modules = config.get(\"lora_config\").get(\"target_modules\"),\n",
        "    lora_alpha = config.get(\"lora_config\").get(\"lora_alpha\"),\n",
        "    lora_dropout = config.get(\"lora_config\").get(\"lora_dropout\"),\n",
        "    bias = config.get(\"lora_config\").get(\"bias\"),\n",
        "    use_gradient_checkpointing = config.get(\"lora_config\").get(\"use_gradient_checkpointing\"),\n",
        "    random_state = 42,\n",
        "    use_rslora = config.get(\"lora_config\").get(\"use_rslora\"),\n",
        "    use_dora = config.get(\"lora_config\").get(\"use_dora\"),\n",
        "    loftq_config = config.get(\"lora_config\").get(\"loftq_config\"),\n",
        ")\n",
        "\n",
        "# Loading the training dataset\n",
        "dataset_train = load_dataset(\n",
        "    config.get(\"training_dataset\").get(\"name\"),\n",
        "    split = config.get(\"training_dataset\").get(\"split\")\n",
        ")\n",
        "\n",
        "# BEFORE the trainer, add this formatting function:\n",
        "def formatting_func(examples):\n",
        "    \"\"\"Format examples for training - must return a list\"\"\"\n",
        "    text_field = config.get(\"training_dataset\").get(\"input_field\")\n",
        "\n",
        "    # If examples is a batch (dict with lists)\n",
        "    if isinstance(examples[text_field], list):\n",
        "        return examples[text_field]\n",
        "    # If examples is a single item\n",
        "    else:\n",
        "        return [examples[text_field]]\n",
        "\n",
        "# Then your trainer:\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    train_dataset = dataset_train,\n",
        "    processing_class = tokenizer,\n",
        "    formatting_func = formatting_func,\n",
        "    dataset_num_proc = 2,\n",
        "    packing = False,\n",
        "    args = SFTConfig(\n",
        "        max_seq_length = config.get(\"model_config\").get(\"max_seq_length\"),\n",
        "        per_device_train_batch_size = config.get(\"training_config\").get(\"per_device_train_batch_size\"),\n",
        "        gradient_accumulation_steps = config.get(\"training_config\").get(\"gradient_accumulation_steps\"),\n",
        "        warmup_steps = config.get(\"training_config\").get(\"warmup_steps\"),\n",
        "        max_steps = config.get(\"training_config\").get(\"max_steps\"),\n",
        "        num_train_epochs = config.get(\"training_config\").get(\"num_train_epochs\"),\n",
        "        learning_rate = config.get(\"training_config\").get(\"learning_rate\"),\n",
        "        fp16 = config.get(\"training_config\").get(\"fp16\"),\n",
        "        bf16 = config.get(\"training_config\").get(\"bf16\"),\n",
        "        logging_steps = config.get(\"training_config\").get(\"logging_steps\"),\n",
        "        optim = config.get(\"training_config\").get(\"optim\"),\n",
        "        weight_decay = config.get(\"training_config\").get(\"weight_decay\"),\n",
        "        lr_scheduler_type = config.get(\"training_config\").get(\"lr_scheduler_type\"),\n",
        "        seed = 42,\n",
        "        output_dir = config.get(\"training_config\").get(\"output_dir\"),\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gAXwqvqbd71b"
      },
      "outputs": [],
      "source": [
        "# Memory statistics before training\n",
        "gpu_statistics = torch.cuda.get_device_properties(0)\n",
        "reserved_memory = round(torch.cuda.max_memory_reserved() / 1024**3, 2)\n",
        "max_memory = round(gpu_statistics.total_memory / 1024**3, 2)\n",
        "print(f\"Reserved Memory: {reserved_memory}GB\")\n",
        "print(f\"Max Memory: {max_memory}GB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yI9mEQ7ZOUx2"
      },
      "outputs": [],
      "source": [
        "# Training the model\n",
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s77_uaq7d71c"
      },
      "outputs": [],
      "source": [
        "# Memory statistics after training\n",
        "used_memory = round(torch.cuda.max_memory_allocated() / 1024**3, 2)\n",
        "used_memory_lora = round(used_memory - reserved_memory, 2)\n",
        "used_memory_persentage = round((used_memory / max_memory) * 100, 2)\n",
        "used_memory_lora_persentage = round((used_memory_lora / max_memory) * 100, 2)\n",
        "print(f\"Used Memory: {used_memory}GB ({used_memory_persentage}%)\")\n",
        "print(f\"Used Memory for training(fine-tuning) LoRA: {used_memory_lora}GB ({used_memory_lora_persentage}%)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VPaDazQ2d71c"
      },
      "outputs": [],
      "source": [
        "# Saving the trainer stats\n",
        "with open(\"trainer_stats.json\", \"w\") as f:\n",
        "    json.dump(trainer_stats, f, indent=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-1HtsRpVnHTj"
      },
      "outputs": [],
      "source": [
        "# Locally saving the model and pushing it to the Hugging Face Hub (only LoRA adapters)\n",
        "model.save_pretrained(config.get(\"model_config\").get(\"finetuned_model\"))\n",
        "model.push_to_hub(config.get(\"model_config\").get(\"finetuned_model\"), tokenizer = tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hdf4NcuYd71c"
      },
      "outputs": [],
      "source": [
        "# Saving the model using merged_16bit(float16), merged_4bit(int4) or quantization options(q8_0, q4_k_m, q5_k_m)...\n",
        "model.save_pretrained_merged(config.get(\"model_config\").get(\"finetuned_model\"), tokenizer, save_method = \"merged_16bit\",)\n",
        "repo_name = config.get(\"model_config\").get(\"finetuned_model\")\n",
        "print(f\"Pushing to: {repo_name}\")  # Check what it prints\n",
        "\n",
        "model.push_to_hub_merged(\n",
        "    repo_name,  # Should be \"username/model-name\"\n",
        "    tokenizer,\n",
        "    save_method = \"merged_16bit\"\n",
        ")\n",
        "model.save_pretrained_merged(config.get(\"model_config\").get(\"finetuned_model\"), tokenizer, save_method = \"merged_4bit\",)\n",
        "model.push_to_hub_merged(config.get(\"model_config\").get(\"finetuned_model\"), tokenizer, save_method = \"merged_4bit\")\n",
        "\n",
        "model.save_pretrained_gguf(config.get(\"model_config\").get(\"finetuned_model\"), tokenizer)\n",
        "model.push_to_hub_gguf(config.get(\"model_config\").get(\"finetuned_model\"), tokenizer)\n",
        "\n",
        "model.save_pretrained_gguf(config.get(\"model_config\").get(\"finetuned_model\"), tokenizer, quantization_method = \"f16\")\n",
        "model.push_to_hub_gguf(config.get(\"model_config\").get(\"finetuned_model\"), tokenizer, quantization_method = \"f16\")\n",
        "\n",
        "model.save_pretrained_gguf(config.get(\"model_config\").get(\"finetuned_model\"), tokenizer, quantization_method = \"q4_k_m\")\n",
        "model.push_to_hub_gguf(config.get(\"model_config\").get(\"finetuned_model\"), tokenizer, quantization_method = \"q4_k_m\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ozVcalyP_JLs"
      },
      "outputs": [],
      "source": [
        "# Loading the fine-tuned model and the tokenizer for inference\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name = config.get(\"model_config\").get(\"finetuned_model\"),\n",
        "        max_seq_length = config.get(\"model_config\").get(\"max_seq_length\"),\n",
        "        dtype = config.get(\"model_config\").get(\"dtype\"),\n",
        "        load_in_4bit = config.get(\"model_config\").get(\"load_in_4bit\"),\n",
        "    )\n",
        "\n",
        "# Using FastLanguageModel for fast inference\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "# Tokenizing the input and generating the output\n",
        "inputs = tokenizer(\n",
        "[\n",
        "    \"<start_of_turn>user Answer the question truthfully, you are a medical professional. This is the question: Can you provide an overview of the lung's squamous cell carcinoma?<end_of_turn>\"\n",
        "], return_tensors = \"pt\").to(\"cuda\")\n",
        "outputs = model.generate(**inputs, max_new_tokens = 256, use_cache = True)\n",
        "tokenizer.decode(outputs, skip_special_tokens = True)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}